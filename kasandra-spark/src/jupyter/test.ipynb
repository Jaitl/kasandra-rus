{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146 new artifacts in macro\n",
      "146 new artifacts in runtime\n",
      "146 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkHome\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/data/spark/spark-1.6.1-bin-hadoop2.6\"\u001b[0m\n",
       "\u001b[36msparkAssembly\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"/data/spark/spark-1.6.1-bin-hadoop2.6/lib/spark-assembly-1.6.1-hadoop2.6.0.jar\"\u001b[0m\n",
       "\u001b[36msparkMaster\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"spark://localhost:7077\"\u001b[0m\n",
       "\u001b[36msparkVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"1.6.1\"\u001b[0m\n",
       "\u001b[36mscalaVersion\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"2.11.8\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkHome = \"/data/spark/spark-1.6.1-bin-hadoop2.6\"\n",
    "val sparkAssembly = s\"$sparkHome/lib/spark-assembly-1.6.1-hadoop2.6.0.jar\"\n",
    "val sparkMaster = \"spark://localhost:7077\"\n",
    "val sparkVersion = \"1.6.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString\n",
    "\n",
    "classpath.add(\n",
    "    \"com.github.alexarchambault.ammonium\" % \"spark_1.6.1_2.11.8\" % \"0.4.0-M6-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:",
      "org.apache.spark.SparkContext.<init>(SparkContext.scala:82)",
      "ammonite.spark.Spark$SparkContext.<init>(Spark.scala:240)",
      "ammonite.spark.Spark.sc(Spark.scala:197)",
      "cmd1$$user$$anonfun$3.apply(Main.scala:34)",
      "cmd1$$user$$anonfun$3.apply(Main.scala:33)",
      "cmd1$$user.<init>(Main.scala:35)",
      "cmd1.<init>(Main.scala:39)",
      "cmd1$.<init>(Main.scala:3)",
      "cmd1$.<clinit>(Main.scala)",
      "cmd1$Main$.$main(Main.scala:79)",
      "cmd1$Main.$main(Main.scala)",
      "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "java.lang.reflect.Method.invoke(Method.java:497)",
      "ammonite.Interpreter$$anonfun$evaluate$1$$anonfun$apply$9.apply(Interpreter.scala:325)",
      "ammonite.Interpreter$.evaluating(Interpreter.scala:291)",
      "ammonite.Interpreter$$anonfun$evaluate$1.apply(Interpreter.scala:325)",
      "ammonite.Interpreter$$anonfun$evaluate$1.apply(Interpreter.scala:324)",
      "ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57) (Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:",
      "org.apache.spark.SparkContext.<init>(SparkContext.scala:82)",
      "ammonite.spark.Spark$SparkContext.<init>(Spark.scala:240)",
      "ammonite.spark.Spark.sc(Spark.scala:197)",
      "cmd1$$user$$anonfun$3.apply(Main.scala:34)",
      "cmd1$$user$$anonfun$3.apply(Main.scala:33)",
      "cmd1$$user.<init>(Main.scala:35)",
      "cmd1.<init>(Main.scala:39)",
      "cmd1$.<init>(Main.scala:3)",
      "cmd1$.<clinit>(Main.scala)",
      "cmd1$Main$.$main(Main.scala:79)",
      "cmd1$Main.$main(Main.scala)",
      "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "java.lang.reflect.Method.invoke(Method.java:497)",
      "ammonite.Interpreter$$anonfun$evaluate$1$$anonfun$apply$9.apply(Interpreter.scala:325)",
      "ammonite.Interpreter$.evaluating(Interpreter.scala:291)",
      "ammonite.Interpreter$$anonfun$evaluate$1.apply(Interpreter.scala:325)",
      "ammonite.Interpreter$$anonfun$evaluate$1.apply(Interpreter.scala:324)",
      "ammonite.InterpreterAction$$anon$1.apply(Interpreter.scala:57))",
      "  org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2257)",
      "  org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2239)",
      "  scala.Option.foreach(Option.scala:257)",
      "  org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2239)",
      "  org.apache.spark.SparkContext$.setActiveContext(SparkContext.scala:2325)",
      "  org.apache.spark.SparkContext.<init>(SparkContext.scala:2197)",
      "  ammonite.spark.Spark$SparkContext.<init>(Spark.scala:240)",
      "  ammonite.spark.Spark.sc(Spark.scala:197)",
      "  cmd2$$user$$anonfun$3.apply(Main.scala:36)",
      "  cmd2$$user$$anonfun$3.apply(Main.scala:35)"
     ]
    }
   ],
   "source": [
    "@transient val Spark = new ammonite.spark.Spark\n",
    "\n",
    "@transient val conf =\n",
    "  Spark\n",
    "    .sparkConf\n",
    "    .setAppName(\"Content Similarity\")\n",
    "    .set(\"spark.home\", sparkHome)\n",
    "\n",
    "@transient val sc = Spark.sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mwords\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = MapPartitionsRDD[92] at flatMap at Main.scala:30\n",
       "\u001b[36mcounts\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m)] = ShuffledRDD[94] at reduceByKey at Main.scala:33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  val words = input.flatMap(line => line.split(\" \"))\n",
    "\n",
    "  val counts = words.map(word => (word, 1)).reduceByKey((a, b) => a + b)\n",
    "  counts.collect().foreach{ case (s, c) => println(s\"word: $s, count: $c\") }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
