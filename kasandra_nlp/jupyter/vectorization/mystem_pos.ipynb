{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"я такой крутой пришел домой\",\n",
    "    \"мама мыла - раму ; рама что то еще делала\",\n",
    "    \"моя рыба рубачит лучше меня\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystem_part = {\n",
    "    'A': 'ADJ', # прилагательное\n",
    "    'ADV': 'ADV', # наречие\n",
    "    'ADVPRO': 'ADV', # местоименное наречие\n",
    "    'ANUM': 'ADJ', # числительное-прилагательное\n",
    "    'APRO': 'DET', # местоимение-прилагательное\n",
    "    'COM': 'ADJ', # часть композита - сложного слова\n",
    "    'CONJ': 'SCONJ', # союз\n",
    "    'INTJ': 'INTJ',\t# междометие\n",
    "    'NUM': 'NUM', # числительное\n",
    "    'PART': 'PART',\t# частица\n",
    "    'PR': 'ADP', # предлог\n",
    "    'S': 'NOUN', # существительное\n",
    "    'SPRO': 'PRON',\n",
    "    'V': 'VERB', # глагол\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_stopwords():\n",
    "    stopwords = set()\n",
    "\n",
    "    with open('../../res/stopwords.txt', mode=\"r\", encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            stopwords.add(line.replace('\\n', ''))\n",
    "\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __mystem_analisys__(mystem, text):\n",
    "    try:   \n",
    "        res = mystem.analyze(text)\n",
    "        stem_res = []\n",
    "        for r in res:\n",
    "            analysis = r['analysis']\n",
    "            try:\n",
    "                gr = analysis[0]\n",
    "                lex = analysis[0]['lex']\n",
    "                parts = gr['gr'].split(\"=\")\n",
    "                parts2 = parts[0].split(\",\")\n",
    "                part = parts2[0]\n",
    "                lex_pas = \"%s_%s\" % (lex, mystem_part[part])\n",
    "                stem_res.append((lex, lex_pas))\n",
    "            except Exception as e:\n",
    "                stem_res.append((r['text'], r['text']))\n",
    "        return stem_res\n",
    "    except Exception as e:\n",
    "        return list(map(lambda x: (x, x), mystem.lemmatize(text)))\n",
    "        \n",
    "    return stem_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nomalize(texts, with_pos=False):\n",
    "    stopwords = load_stopwords()\n",
    "    r = re.compile('^[А-ЯЙа-яй]*$')\n",
    "    mystem = Mystem(entire_input=False)\n",
    "    tokens_corpuse = []\n",
    "    for text in texts:\n",
    "        words = filter(lambda x: r.match(x[0]), __mystem_analisys__(mystem, text))\n",
    "        tokens = filter(lambda w: w[0] not in stopwords, words)\n",
    "        \n",
    "        tokens_res = []\n",
    "        if with_pos:\n",
    "            tokens_res = list(map(lambda x: x[1], tokens))\n",
    "        else:\n",
    "            tokens_res = list(map(lambda x: x[0], tokens))\n",
    "        \n",
    "        tokens_corpuse.append(tokens_res)\n",
    "    return tokens_corpuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_word3vec():\n",
    "    model_path = '/data/gensim/news_0_300_2.bin.gz'\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "    word_vectors.init_sims(replace=True)\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar(word_vectors, word):\n",
    "    try:\n",
    "        sim = word_vectors.most_similar(positive=[w])\n",
    "        return list(map(lambda x: x[0], sim))\n",
    "    except Exception as e:\n",
    "        return [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractSemanticGroup(tokens_matrix, with_pos=False):\n",
    "    start_time = time.time()\n",
    "    word_vectors = init_word3vec()\n",
    "    print(\"init time: %s\" % (time.time() - start_time))\n",
    "    res_seq = []\n",
    "    sem_groups = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for words in tokens_matrix:\n",
    "        res_tokens = []\n",
    "        for token in words:\n",
    "            if token not in sem_groups:\n",
    "                sims = similar(word_vectors, token)\n",
    "                sem_groups[token] = token\n",
    "                res_tokens.append(token)\n",
    "                for sim in sims:\n",
    "                   sem_groups[sim] = token\n",
    "            else:\n",
    "                res_tokens.append(sem_groups[token])\n",
    "        res_seq.append(res_tokens)\n",
    "    print(\"for time: %s\" % (time.time() - start_time))\n",
    "    if not with_pos:\n",
    "        res_seq = map(lambda x: list(map(lambda y: y.split(\"_\")[0], x)), res_seq)\n",
    "\n",
    "    return list(res_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_res = nomalize(texts, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init time: 9.179507970809937\n",
      "for time: 2.4080276489257812e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['крутой', 'приходить', 'домой'],\n",
       " ['мама', 'мыть', 'рама', 'рама', 'делать'],\n",
       " ['рыба', 'рубачить']]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractSemanticGroup(norm_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
