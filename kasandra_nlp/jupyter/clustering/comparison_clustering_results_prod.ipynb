{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение алгоритмов кластеризации по метрикам качества кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "from gensim.sklearn_api import LdaTransformer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_time = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log path: /data/logs/1514117172.log\n"
     ]
    }
   ],
   "source": [
    "log_path = '/data/logs/%s.log' % run_time\n",
    "print(\"log path: %s\" % log_path)\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.INFO)\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('[%(asctime)s] %(levelname)-5s %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "root.addHandler(ch)\n",
    "\n",
    "fh = logging.FileHandler(log_path, 'w', encoding='utf8')\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "root.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# printt = print\n",
    "printt = logging.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных для кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class News:\n",
    "    def __init__(self, id, date, title, content, url, siteType):\n",
    "        self.id = id\n",
    "        self.date = date\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.url = url\n",
    "        self.siteType = siteType\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_str):\n",
    "        json_dict = json.loads(json_str)\n",
    "        return cls(**json_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = []\n",
    "with open('/data/10k.test.normalized.json', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        news.append(News.from_json(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-24 12:06:14,071] INFO  total words: 2740504\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for n in news:\n",
    "    words.extend(n.content.split())\n",
    "counts = Counter(words)\n",
    "one_time = [k for k, v in dict(counts).items() if v < 2 and v > 10000]\n",
    "printt(\"total words: %s\" % (len(words) - len(one_time)))\n",
    "\n",
    "news_content = [x.content for x in news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-24 12:06:14,079] INFO  stop words: 230\n"
     ]
    }
   ],
   "source": [
    "stopwords = one_time\n",
    "with open('/data/stopwords.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        stopwords.append(line)\n",
    "printt(\"stop words: %s\" % (len(stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split()] for document in news_content]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка размеченной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marked_map = {} # (id, label)\n",
    "with open('/data/mark_news.csv', encoding=\"utf8\") as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=';')\n",
    "    for row in spamreader:\n",
    "        marked_map[row[0]] = int(row[3])\n",
    "\n",
    "marked_news = []\n",
    "for n in news:\n",
    "    label = marked_map[n.id]\n",
    "    marked_news.append((n.id, label))\n",
    "    \n",
    "marked_labels = [label for n_id, label in marked_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked_map['170bf9b9-d62d-437e-a75a-cac7b7c9f282']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zip_news(n,l):\n",
    "    return list(map(assign_label_to_news, zip(n, l)))\n",
    "\n",
    "def assign_label_to_news(tuplezz):\n",
    "    (nws, lbl) = tuplezz\n",
    "    nws.label = lbl.item()\n",
    "    return nws\n",
    "\n",
    "def filter_words(text):\n",
    "    words_list = text.split()\n",
    "    newWords = [x for x in words_list if len(x) > 3]\n",
    "    return \" \".join(newWords)\n",
    "\n",
    "def print_clusters(cluster_news, clustre_labels):\n",
    "    newsLabels = zip_news(cluster_news, clustre_labels)\n",
    "    newsLabels = sorted(newsLabels, key=lambda n: n.label)\n",
    "    for label, group in groupby(newsLabels, lambda n: n.label):\n",
    "        groupList = list(group)\n",
    "        printt(\"Cluster: %s, count news: %s, titles:\" % (label, len(groupList)))\n",
    "        for gr in groupList:\n",
    "            printt(\"\\t\" + gr.title)\n",
    "            \n",
    "def print_topics(components, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        printt(\"Topic #%d:\" % topic_idx)\n",
    "        printt(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-24 12:06:16,343] INFO  loading projection weights from /data/gensim/news_0_300_2.bin.gz\n",
      "[2017-12-24 12:06:22,514] INFO  loaded (124590, 300) matrix from /data/gensim/news_0_300_2.bin.gz\n",
      "[2017-12-24 12:06:22,515] INFO  precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "%run /data/jupyter/semantic_group.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(content, ngram_range = (1, 1), max_features = None, min_df = 1, max_df = 1.0):\n",
    "    time_start = time.time()\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, tokenizer=lambda text: text.split(\" \"), stop_words=stopwords, norm='l2', ngram_range=ngram_range, max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(content)\n",
    "    printt(\"vocabulary size: %s\" % len(tfidf_vectorizer.vocabulary_))\n",
    "    printt(\"tf_idf time: %s s\" % (time.time() - time_start))\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(content):\n",
    "    time_start = time.time()\n",
    "    tf_vectorizer = TfidfVectorizer(use_idf=False, tokenizer=lambda text: text.split(\" \"), stop_words=stopwords) # , ngram_range=(1, 3)\n",
    "    tf_matrix = tf_vectorizer.fit_transform(content)\n",
    "    printt(\"vocabulary size: %s\" % len(tf_vectorizer.vocabulary_))\n",
    "    printt(\"tf time: %s s\" % (time.time() - time_start))\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semantic_group(content, ngram_range = (1, 1), min_df = 1, max_df = 1.0):\n",
    "    time_start = time.time()\n",
    "    sem_content = extractSemanticGroup(content)\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, tokenizer=lambda text: text.split(\" \"), stop_words=stopwords, norm='l2', ngram_range=ngram_range, min_df=min_df, max_df=max_df)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sem_content)\n",
    "    printt(\"vocabulary size: %s\" % len(tfidf_vectorizer.vocabulary_))\n",
    "    printt(\"semantic_group time: %s s\" % (time.time() - time_start))\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семантический анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda(corpus, id2word, n):\n",
    "    time_start = time.time()\n",
    "    lda_t = LdaTransformer(num_topics=n, id2word=id2word)\n",
    "    lda_matrix = lda_t.fit_transform(corpus)\n",
    "    normalizer = Normalizer()\n",
    "    norm_matrix = normalizer.fit_transform(lda_matrix)\n",
    "    printt(\"vocabulary size: %s\" % norm_matrix.shape[0])\n",
    "    printt(\"lda-%s, time: %s s\" % (n, time.time() - time_start))\n",
    "    return csr_matrix(norm_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка качества кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(matrix, marked_labels, clustered_labels):\n",
    "    printt(\"V-measure: %0.3f\" % metrics.v_measure_score(marked_labels, clustered_labels))\n",
    "    printt(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(marked_labels, clustered_labels))\n",
    "    printt(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(matrix, clustered_labels, sample_size=1000))\n",
    "    printt(\"Cluster count: %s\" % len(set(clustered_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_file(matrix, marked_labels, clustered_labels, file):\n",
    "    file.write(\"Homogeneity: %0.3f\\n\" % metrics.homogeneity_score(marked_labels, clustered_labels))\n",
    "    file.write(\"Completeness: %0.3f\\n\" % metrics.completeness_score(marked_labels, clustered_labels))\n",
    "    file.write(\"V-measure: %0.3f\\n\" % metrics.v_measure_score(marked_labels, clustered_labels))\n",
    "    file.write(\"Adjusted Rand-Index: %.3f\\n\" % metrics.adjusted_rand_score(marked_labels, clustered_labels))\n",
    "    file.write(\"Silhouette Coefficient: %0.3f\\n\" % metrics.silhouette_score(matrix, clustered_labels, sample_size=1000))\n",
    "    file.write(\"Cluster count: %s\\n\" % len(set(clustered_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_clusters_file(cluster_news, clustre_labels, file):\n",
    "    newsLabels = zip_news(cluster_news, clustre_labels)\n",
    "    newsLabels = sorted(newsLabels, key=lambda n: n.label)\n",
    "    for label, group in groupby(newsLabels, lambda n: n.label):\n",
    "        groupList = list(group)\n",
    "        file.write(\"Cluster: %s, count news: %s, titles:\\n\" % (label, len(groupList)))\n",
    "        for gr in groupList:\n",
    "            file.write(\"\\t\" + gr.title + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iter = 100 # 100\n",
    "n_clusters = 130 # Количество кластеров\n",
    "min_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-24 12:06:29,727] INFO  adding document #0 to Dictionary(0 unique tokens: [])\n",
      "[2017-12-24 12:06:32,549] INFO  built Dictionary(43455 unique tokens: ['активность', 'ар', 'боевик', 'ввс', 'вести']...) from 10000 documents (total 2717157 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-24 12:06:39,120] INFO  using symmetric alpha at 0.0002\n",
      "[2017-12-24 12:06:39,122] INFO  using symmetric eta at 0.0002\n",
      "[2017-12-24 12:06:39,129] INFO  using serial LDA version on this node\n",
      "[2017-12-24 12:46:08,446] INFO  running online (single-pass) LDA training, 5000 topics, 1 passes over the supplied corpus of 10000 documents, updating model once every 2000 documents, evaluating perplexity every 10000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "[2017-12-24 12:46:08,447] WARNING too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "[2017-12-24 12:46:08,447] INFO  PROGRESS: pass 0, at document #2000/10000\n",
      "[2017-12-24 12:49:27,605] INFO  merging changes from 2000 documents into a model of 10000 documents\n",
      "[2017-12-24 12:50:07,700] INFO  topic #1492 (0.000): 0.013*\"город\" + 0.013*\"год\" + 0.009*\"урал\" + 0.009*\"система\" + 0.008*\"инвест\" + 0.008*\"москва\" + 0.007*\"немец\" + 0.007*\"россия\" + 0.006*\"башнефть\" + 0.006*\"цена\"\n",
      "[2017-12-24 12:50:07,701] INFO  topic #2041 (0.000): 0.017*\"самолет\" + 0.014*\"человек\" + 0.014*\"тайвань\" + 0.011*\"год\" + 0.010*\"спасатель\" + 0.009*\"китайский\" + 0.009*\"падение\" + 0.008*\"лайнер\" + 0.008*\"становиться\" + 0.008*\"данные\"\n",
      "[2017-12-24 12:50:07,702] INFO  topic #2592 (0.000): 0.016*\"фэллона\" + 0.014*\"россия\" + 0.011*\"ядерный\" + 0.010*\"украина\" + 0.010*\"великобритания\" + 0.010*\"государство\" + 0.010*\"российский\" + 0.009*\"сила\" + 0.008*\"слово\" + 0.008*\"президент\"\n",
      "[2017-12-24 12:50:07,703] INFO  topic #4770 (0.000): 0.020*\"глава\" + 0.017*\"россия\" + 0.015*\"мид\" + 0.013*\"встреча\" + 0.012*\"санкция\" + 0.011*\"украина\" + 0.011*\"неделя\" + 0.011*\"февраль\" + 0.010*\"переговоры\" + 0.010*\"меркель\"\n",
      "[2017-12-24 12:50:07,704] INFO  topic #938 (0.000): 0.038*\"безработица\" + 0.030*\"уровень\" + 0.015*\"человек\" + 0.015*\"год\" + 0.013*\"ожидание\" + 0.012*\"население\" + 0.011*\"пока\" + 0.010*\"труд\" + 0.009*\"рынок\" + 0.009*\"безработный\"\n",
      "[2017-12-24 12:50:08,130] INFO  topic diff=4868.460938, rho=1.000000\n",
      "[2017-12-24 12:50:08,135] INFO  PROGRESS: pass 0, at document #4000/10000\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "[2017-12-24 12:53:19,824] INFO  merging changes from 2000 documents into a model of 10000 documents\n",
      "[2017-12-24 12:53:51,394] INFO  topic #266 (0.000): 0.053*\"смерть\" + 0.045*\"гонка\" + 0.035*\"алина\" + 0.034*\"биатлонистка\" + 0.030*\"спортсменка\" + 0.027*\"время\" + 0.024*\"тюмень\" + 0.022*\"индивидуальный\" + 0.021*\"сердце\" + 0.020*\"биатлонист\"\n",
      "[2017-12-24 12:53:51,395] INFO  topic #98 (0.000): 0.011*\"крым\" + 0.011*\"конкурс\" + 0.009*\"украина\" + 0.008*\"продолжать\" + 0.008*\"федеральный\" + 0.008*\"дебальцево\" + 0.008*\"донбасс\" + 0.008*\"заявлять\" + 0.007*\"частота\" + 0.007*\"радио\"\n",
      "[2017-12-24 12:53:51,396] INFO  topic #1283 (0.000): 0.032*\"жест\" + 0.027*\"прикладной\" + 0.021*\"путин\" + 0.018*\"украина\" + 0.013*\"мгу\" + 0.013*\"овладение\" + 0.012*\"рукопожатие\" + 0.012*\"кафедра\" + 0.012*\"владимир\" + 0.012*\"владимирович\"\n",
      "[2017-12-24 12:53:51,397] INFO  topic #4366 (0.000): 0.011*\"год\" + 0.007*\"цена\" + 0.006*\"процент\" + 0.006*\"дебальцево\" + 0.005*\"стрелок\" + 0.005*\"рост\" + 0.005*\"мочь\" + 0.005*\"инфляция\" + 0.005*\"украинский\" + 0.005*\"россия\"\n",
      "[2017-12-24 12:53:51,398] INFO  topic #1383 (0.000): 0.166*\"монастырь\" + 0.045*\"христос\" + 0.043*\"спаситель\" + 0.037*\"храм\" + 0.031*\"православный\" + 0.027*\"день\" + 0.023*\"великий\" + 0.023*\"праздник\" + 0.019*\"традиция\" + 0.019*\"сегодня\"\n",
      "[2017-12-24 12:53:51,821] INFO  topic diff=inf, rho=0.707107\n",
      "[2017-12-24 12:53:51,826] INFO  PROGRESS: pass 0, at document #6000/10000\n",
      "[2017-12-24 12:57:00,952] INFO  merging changes from 2000 documents into a model of 10000 documents\n",
      "[2017-12-24 12:57:31,110] INFO  topic #2860 (0.000): 0.032*\"начисляться\" + 0.020*\"вдвое\" + 0.016*\"транспорт\" + 0.012*\"цена\" + 0.007*\"билет\" + 0.007*\"процент\" + 0.006*\"перевозка\" + 0.006*\"регистрировать\" + 0.006*\"гривна\" + 0.005*\"мочь\"\n",
      "[2017-12-24 12:57:31,111] INFO  topic #2847 (0.000): 0.062*\"николай\" + 0.051*\"царский\" + 0.047*\"романов\" + 0.041*\"написание\" + 0.038*\"император\" + 0.022*\"канон\" + 0.021*\"дворец\" + 0.020*\"год\" + 0.018*\"родина\" + 0.018*\"миллион\"\n",
      "[2017-12-24 12:57:31,112] INFO  topic #3947 (0.000): 0.154*\"заявление\" + 0.137*\"мид\" + 0.064*\"россия\" + 0.053*\"снг\" + 0.052*\"лавров\" + 0.052*\"гражданин\" + 0.048*\"въезд\" + 0.043*\"проинформировать\" + 0.032*\"яценюк\" + 0.032*\"россиянин\"\n",
      "[2017-12-24 12:57:31,113] INFO  topic #512 (0.000): 0.012*\"препарат\" + 0.010*\"год\" + 0.009*\"интернет\" + 0.008*\"министерство\" + 0.007*\"закон\" + 0.007*\"глобальный\" + 0.007*\"лицензия\" + 0.007*\"российский\" + 0.006*\"мочь\" + 0.006*\"культура\"\n",
      "[2017-12-24 12:57:31,114] INFO  topic #1107 (0.000): 0.010*\"эдуард\" + 0.008*\"басурин\" + 0.007*\"украинский\" + 0.006*\"представитель\" + 0.006*\"киев\" + 0.005*\"днр\" + 0.005*\"позиция\" + 0.005*\"тяжелый\" + 0.005*\"донецкий\" + 0.005*\"силовик\"\n",
      "[2017-12-24 12:57:31,537] INFO  topic diff=inf, rho=0.577350\n",
      "[2017-12-24 12:57:31,542] INFO  PROGRESS: pass 0, at document #8000/10000\n",
      "[2017-12-24 13:00:43,123] INFO  merging changes from 2000 documents into a model of 10000 documents\n",
      "[2017-12-24 13:01:12,543] INFO  topic #483 (0.000): 0.011*\"год\" + 0.005*\"зимний\" + 0.005*\"москва\" + 0.005*\"украина\" + 0.005*\"февраль\" + 0.004*\"доренко\" + 0.004*\"отменять\" + 0.004*\"срок\" + 0.004*\"бизнес\" + 0.004*\"говорить\"\n",
      "[2017-12-24 13:01:12,544] INFO  topic #1033 (0.000): 0.000*\"просушивать\" + 0.000*\"руккола\" + 0.000*\"нарубать\" + 0.000*\"отбивной\" + 0.000*\"передерживать\" + 0.000*\"перчить\" + 0.000*\"подсыпать\" + 0.000*\"полукольцо\" + 0.000*\"простокваша\" + 0.000*\"ломкий\"\n",
      "[2017-12-24 13:01:12,545] INFO  topic #4225 (0.000): 0.024*\"фильм\" + 0.017*\"кино\" + 0.013*\"звезда\" + 0.012*\"продвижение\" + 0.012*\"год\" + 0.011*\"снимать\" + 0.010*\"рано\" + 0.010*\"овечка\" + 0.008*\"канал\" + 0.008*\"рейтинг\"\n",
      "[2017-12-24 13:01:12,546] INFO  topic #2328 (0.000): 0.198*\"выживать\" + 0.117*\"связываться\" + 0.045*\"наиболее\" + 0.027*\"указывать\" + 0.025*\"диспетчер\" + 0.025*\"снижаться\" + 0.024*\"гендиректор\" + 0.014*\"авиакатастрофа\" + 0.014*\"расследование\" + 0.013*\"течение\"\n",
      "[2017-12-24 13:01:12,547] INFO  topic #248 (0.000): 0.055*\"продуктивность\" + 0.012*\"рост\" + 0.009*\"производительность\" + 0.009*\"рабочий\" + 0.008*\"место\" + 0.006*\"россия\" + 0.005*\"многий\" + 0.005*\"социальный\" + 0.004*\"работать\" + 0.004*\"мочь\"\n",
      "[2017-12-24 13:01:12,973] INFO  topic diff=inf, rho=0.500000\n",
      "[2017-12-24 13:04:48,875] INFO  -205.298 per-word bound, 63202235841577928738889917197135527698724675845866800931667968.0 perplexity estimate based on a held-out corpus of 2000 documents with 489183 words\n",
      "[2017-12-24 13:04:48,876] INFO  PROGRESS: pass 0, at document #10000/10000\n",
      "[2017-12-24 13:07:54,448] INFO  merging changes from 2000 documents into a model of 10000 documents\n",
      "[2017-12-24 13:08:23,574] INFO  topic #3604 (0.000): 0.015*\"рубль\" + 0.013*\"тулеев\" + 0.011*\"губернатор\" + 0.008*\"россия\" + 0.008*\"собеседник\" + 0.008*\"курс\" + 0.007*\"неверов\" + 0.007*\"доллар\" + 0.006*\"кемеровский\" + 0.006*\"ставка\"\n",
      "[2017-12-24 13:08:23,576] INFO  topic #810 (0.000): 0.387*\"смеяться\" + 0.038*\"соня\" + 0.014*\"ребенок\" + 0.011*\"детский\" + 0.008*\"мочь\" + 0.007*\"вообще\" + 0.006*\"платить\" + 0.006*\"школа\" + 0.006*\"суд\" + 0.006*\"пугачева\"\n",
      "[2017-12-24 13:08:23,577] INFO  topic #1824 (0.000): 0.198*\"датировать\" + 0.112*\"нечаянно\" + 0.081*\"компьютерщик\" + 0.075*\"багамы\" + 0.011*\"год\" + 0.010*\"загодя\" + 0.007*\"отрицательный\" + 0.006*\"мочь\" + 0.005*\"депп\" + 0.005*\"процентный\"\n",
      "[2017-12-24 13:08:23,578] INFO  topic #4493 (0.000): 0.152*\"мало\" + 0.149*\"координация\" + 0.110*\"сцкк\" + 0.065*\"совместный\" + 0.051*\"центр\" + 0.050*\"отвод\" + 0.040*\"контроль\" + 0.023*\"представитель\" + 0.020*\"лишаться\" + 0.019*\"тяжелый\"\n",
      "[2017-12-24 13:08:23,579] INFO  topic #2801 (0.000): 0.519*\"океан\" + 0.318*\"тасс\" + 0.034*\"норвежский\" + 0.020*\"газета\" + 0.019*\"передавать\" + 0.018*\"граница\" + 0.016*\"королевство\" + 0.014*\"заявлять\" + 0.007*\"необходимый\" + 0.001*\"главный\"\n",
      "[2017-12-24 13:08:24,018] INFO  topic diff=inf, rho=0.447214\n",
      "[2017-12-24 13:21:51,609] INFO  vocabulary size: 10000\n",
      "[2017-12-24 13:21:51,610] INFO  lda-5000, time: 4512.49151134491 s\n"
     ]
    }
   ],
   "source": [
    "vectorization = {\n",
    "    # \"lda_1000\": lda(corpus, dictionary, 1000),\n",
    "    \"lda_5000\": lda(corpus, dictionary, 5000),\n",
    "    # \"lda_10000\": lda(corpus, dictionary, 10000),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization = {\n",
    "    # \"tf-idf\": tf_idf(news_content, (1, 1), None, 2, 0.9),\n",
    "    \"tf-idf_ngram_1_2\": tf_idf(news_content, (1, 2), None, 2, 0.9),\n",
    "    # \"semantic_group\": semantic_group(news_content, (1, 1), 2, 0.9),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparce matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clasterization = {\n",
    "    \"kmeans\": lambda: KMeans(n_clusters=n_clusters),\n",
    "    \"affinityPropagation\": lambda: AffinityPropagation(),\n",
    "    \"birch\": lambda: Birch(n_clusters=n_clusters),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clasterization = {\n",
    "    \"birch\": lambda: Birch(n_clusters=None)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clasterization = {\n",
    "    \"agglomerativeClustering\": lambda: AgglomerativeClustering(n_clusters=n_clusters)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Console output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cl_name, cl_alg in clasterization.items():\n",
    "    for vec_name, vec_matrix in vectorization.items():\n",
    "        try:\n",
    "            printt(\"----\")\n",
    "            printt(\"%s %s\" % (cl_name, vec_name))\n",
    "             alg = cl_alg()\n",
    "            result_matrix = alg.fit(vec_matrix)\n",
    "            # result_matrix = cl_alg().fit(vec_matrix.toarray())\n",
    "            labels = result_matrix.labels_\n",
    "            score(vec_matrix, marked_labels, labels)\n",
    "            printt(\"%s %s: %s second\" % (cl_name, vec_name, time.time() - start_time))\n",
    "            \n",
    "            del labels\n",
    "            del result_matrix\n",
    "            del alg\n",
    "            gc.collect()\n",
    "        except Exception as ex:\n",
    "            printt(\"ERROR: %s, %s\" % (type(ex), str(ex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File output\n",
    "Сохраняет результаты кластеризации вместе с примерами кластеризации в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/data/results/clustering/%s\" % (run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-24 13:21:52,202] INFO  kmeans lda_5000\n",
      "[2017-12-24 13:22:39,484] INFO  time: 47.28091907501221\n",
      "[2017-12-24 13:22:39,485] INFO  path: /data/results/clustering/1514117172/kmeans_lda_5000.txt\n",
      "[2017-12-24 13:22:40,138] INFO  affinityPropagation lda_5000\n",
      "[2017-12-24 13:24:22,870] INFO  time: 102.73041367530823\n",
      "[2017-12-24 13:24:22,871] INFO  path: /data/results/clustering/1514117172/affinityPropagation_lda_5000.txt\n",
      "[2017-12-24 13:24:30,088] INFO  birch lda_5000\n",
      "[2017-12-24 13:26:02,856] INFO  time: 92.76637101173401\n",
      "[2017-12-24 13:26:02,857] INFO  path: /data/results/clustering/1514117172/birch_lda_5000.txt\n"
     ]
    }
   ],
   "source": [
    "for cl_name, cl_alg in clasterization.items():\n",
    "    for vec_name, vec_matrix in vectorization.items():\n",
    "        printt(\"%s %s\" % (cl_name, vec_name))\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            alg = cl_alg()\n",
    "            result_matrix = alg.fit(vec_matrix)\n",
    "            # result_matrix = cl_alg().fit(vec_matrix.toarray())\n",
    "            labels = result_matrix.labels_\n",
    "            printt(\"time: %s\" % (time.time() - start_time))\n",
    "            file_name = \"/%s_%s.txt\" % (cl_name, vec_name)\n",
    "            printt(\"path: %s\" %(path + file_name))\n",
    "            \n",
    "            with open(path + file_name, mode='w', encoding='utf-8') as f:\n",
    "                f.write(\"%s %s\\n\" % (cl_name, vec_name))\n",
    "                score_file(vec_matrix, marked_labels, labels, f)\n",
    "                f.write(\"time: %s\\n\" % (time.time() - start_time))\n",
    "                print_clusters_file(news, labels, f)\n",
    "                \n",
    "            del labels\n",
    "            del result_matrix\n",
    "            del alg\n",
    "            gc.collect()\n",
    "        except Exception as ex:\n",
    "            printt(\"ERROR: %s, %s\" % (type(ex), str(ex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clasterization2 = {\n",
    "    \"agglomerativeClustering\": lambda: AgglomerativeClustering(n_clusters=n_clusters)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-24 13:26:03,541] INFO  agglomerativeClustering lda_5000\n",
      "[2017-12-24 13:29:11,979] INFO  time: 188.43630361557007\n",
      "[2017-12-24 13:29:11,980] INFO  path: /data/results/clustering/1514117172/agglomerativeClustering_lda_5000.txt\n"
     ]
    }
   ],
   "source": [
    "for cl_name, cl_alg in clasterization2.items():\n",
    "    for vec_name, vec_matrix in vectorization.items():\n",
    "        printt(\"%s %s\" % (cl_name, vec_name))\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            alg = cl_alg()\n",
    "            # result_matrix = alg.fit(vec_matrix)\n",
    "            result_matrix = cl_alg().fit(vec_matrix.toarray())\n",
    "            labels = result_matrix.labels_\n",
    "            printt(\"time: %s\" % (time.time() - start_time))\n",
    "            file_name = \"/%s_%s.txt\" % (cl_name, vec_name)\n",
    "            printt(\"path: %s\" %(path + file_name))\n",
    "            \n",
    "            with open(path + file_name, mode='w', encoding='utf-8') as f:\n",
    "                f.write(\"%s %s\\n\" % (cl_name, vec_name))\n",
    "                score_file(vec_matrix, marked_labels, labels, f)\n",
    "                f.write(\"time: %s\\n\" % (time.time() - start_time))\n",
    "                print_clusters_file(news, labels, f)\n",
    "                \n",
    "            del labels\n",
    "            del result_matrix\n",
    "            del alg\n",
    "            gc.collect()\n",
    "        except Exception as ex:\n",
    "            printt(\"ERROR: %s, %s\" % (type(ex), str(ex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "matrix = csr_matrix(vectorization[\"tf-idf\"])\n",
    "dbscan = DBSCAN(eps=1.1, min_samples=min_samples, metric=\"euclidean\")\n",
    "dbscan.fit(matrix)\n",
    "labels = dbscan.labels_\n",
    "score(matrix, marked_labels, labels)\n",
    "printt(\"dbscan time: %s s\" % (time.time() - time_start))\n",
    "\n",
    "file_name = \"/%s_%s.txt\" % (\"dbscan\", \"tf-idf\")\n",
    "with open(path + file_name, mode='w', encoding='utf-8') as f:\n",
    "        f.write(\"%s %s\\n\" % (\"dbscan\", \"tf-idf\"))\n",
    "        score_file(matrix, marked_labels, labels, f)\n",
    "        f.write(\"time: %s\\n\" % (time.time() - time_start))\n",
    "        print_clusters_file(news, labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "matrix = csr_matrix(vectorization[\"tf-idf_ngram_1_2\"])\n",
    "dbscan = DBSCAN(eps=1.2, min_samples=min_samples, metric=\"euclidean\")\n",
    "dbscan.fit(matrix)\n",
    "labels = dbscan.labels_\n",
    "score(matrix, marked_labels, labels)\n",
    "printt(\"dbscan time: %s s\" % (time.time() - time_start))\n",
    "\n",
    "file_name = \"/%s_%s.txt\" % (\"dbscan\", \"idf_ngram_1_2\")\n",
    "with open(path + file_name, mode='w', encoding='utf-8') as f:\n",
    "        f.write(\"%s %s\\n\" % (\"dbscan\", \"idf_ngram_1_2\"))\n",
    "        score_file(matrix, marked_labels, labels, f)\n",
    "        f.write(\"time: %s\\n\" % (time.time() - time_start))\n",
    "        print_clusters_file(news, labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "matrix = vectorization[\"semantic_group\"]\n",
    "dbscan = DBSCAN(eps=1, min_samples=min_samples, metric=\"euclidean\")\n",
    "dbscan.fit(matrix)\n",
    "labels = dbscan.labels_\n",
    "score(matrix, marked_labels, labels)\n",
    "printt(\"dbscan time: %s s\" % (time.time() - time_start))\n",
    "\n",
    "file_name = \"/%s_%s.txt\" % (\"dbscan\", \"semantic_group\")\n",
    "with open(path + file_name, mode='w', encoding='utf-8') as f:\n",
    "        f.write(\"%s %s\\n\" % (\"dbscan\", \"semantic_group\"))\n",
    "        score_file(matrix, marked_labels, labels, f)\n",
    "        f.write(\"time: %s\\n\" % (time.time() - time_start))\n",
    "        print_clusters_file(news, labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "matrix = vectorization[\"lda_1000\"]\n",
    "dbscan = DBSCAN(eps=0.62, min_samples=min_samples, metric=\"euclidean\")\n",
    "dbscan.fit(matrix)\n",
    "labels = dbscan.labels_\n",
    "score(matrix, marked_labels, labels)\n",
    "printt(\"dbscan time: %s s\" % (time.time() - time_start))\n",
    "\n",
    "file_name = \"/%s_%s.txt\" % (\"dbscan\", \"lda_1000\")\n",
    "with open(path + file_name, mode='w', encoding='utf-8') as f:\n",
    "        f.write(\"%s %s\\n\" % (\"dbscan\", \"lda_1000\"))\n",
    "        score_file(matrix, marked_labels, labels, f)\n",
    "        f.write(\"time: %s\\n\" % (time.time() - time_start))\n",
    "        print_clusters_file(news, labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-24 14:22:16,508] INFO  V-measure: 0.413\n",
      "[2017-12-24 14:22:16,513] INFO  Adjusted Rand-Index: 0.011\n",
      "[2017-12-24 14:22:16,860] INFO  Silhouette Coefficient: -0.033\n",
      "[2017-12-24 14:22:16,862] INFO  Cluster count: 196\n",
      "[2017-12-24 14:22:16,863] INFO  dbscan time: 1.8217382431030273 s\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "matrix = vectorization[\"lda_5000\"]\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=min_samples, metric=\"euclidean\")\n",
    "dbscan.fit(matrix)\n",
    "labels = dbscan.labels_\n",
    "score(matrix, marked_labels, labels)\n",
    "printt(\"dbscan time: %s s\" % (time.time() - time_start))\n",
    "\n",
    "file_name = \"/%s_%s.txt\" % (\"dbscan\", \"lda_5000\")\n",
    "with open(path + file_name, mode='w', encoding='utf-8') as f:\n",
    "        f.write(\"%s %s\\n\" % (\"dbscan\", \"lda_5000\"))\n",
    "        score_file(matrix, marked_labels, labels, f)\n",
    "        f.write(\"time: %s\\n\" % (time.time() - time_start))\n",
    "        print_clusters_file(news, labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
