{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных для кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class News:\n",
    "    def __init__(self, id, date, title, content, url, siteType):\n",
    "        self.id = id\n",
    "        self.date = date\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.url = url\n",
    "        self.siteType = siteType\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_str):\n",
    "        json_dict = json.loads(json_str)\n",
    "        return cls(**json_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = []\n",
    "with open('/data/kasandra/year/all.normalized.json', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        news.append(News.from_json(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for n in news:\n",
    "    words.extend(n.content.split())\n",
    "counts = Counter(words)\n",
    "one_time = [k for k, v in dict(counts).items() if v == 1]\n",
    "print(\"total words: %s\" % (len(words) - len(one_time)))\n",
    "\n",
    "news_content = [x.content for x in news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(one_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка размеченной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marked_map = {} # (id, label)\n",
    "with open('eggs.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        marked_map[row[0]] = int(row[3])\n",
    "\n",
    "marked_news = []\n",
    "for n in news:\n",
    "    label = marked_map[n.id]\n",
    "    marked_news.append((n.id, label))\n",
    "    \n",
    "marked_labels = [label for n_id, label in marked_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zip_news(n,l):\n",
    "    return list(map(assign_label_to_news, zip(n, l)))\n",
    "\n",
    "def assign_label_to_news(tuplezz):\n",
    "    (nws, lbl) = tuplezz\n",
    "    nws.label = lbl.item()\n",
    "    return nws\n",
    "\n",
    "def filter_words(text):\n",
    "    words_list = text.split()\n",
    "    newWords = [x for x in words_list if len(x) > 3]\n",
    "    return \" \".join(newWords)\n",
    "\n",
    "def print_clusters(cluster_news, clustre_labels):\n",
    "    newsLabels = zip_news(cluster_news, clustre_labels)\n",
    "    newsLabels = sorted(newsLabels, key=lambda n: n.label)\n",
    "    for label, group in groupby(newsLabels, lambda n: n.label):\n",
    "        groupList = list(group)\n",
    "        print(\"Cluster: %s, count news: %s, titles:\" % (label, len(groupList)))\n",
    "        for gr in groupList:\n",
    "            print(\"\\t\" + gr.title)\n",
    "            \n",
    "def print_topics(components, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(content):\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, tokenizer=lambda text: text.split(\" \"), stop_words=stopwords) # , ngram_range=(1, 3)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(content)\n",
    "    print(\"vocabulary size: %s\" % len(tfidf_vectorizer.vocabulary_))\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda(matrix, n):\n",
    "    lda = LatentDirichletAllocation(n_topics=n, max_iter=100, learning_method='online', learning_offset=50.)\n",
    "    lda_matrix = lda.fit_transform(matrix)\n",
    "    return lda_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(matrix, eps, samples, n_jobs):\n",
    "    db = DBSCAN(eps=eps, min_samples=samples, n_jobs=n_jobs).fit(matrix)\n",
    "    labels = db.labels_\n",
    "    print('count clusters: %d' % (len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)))\n",
    "    labels = db.labels_\n",
    "    print(\"-1: %s, 0: %s\" % (labels.tolist().count(-1), labels.tolist().count(0)))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(matrix, n, n_jobs)\n",
    "    km = KMeans(n_clusters=n, n_jobs=n_jobs).fit(matrix)\n",
    "    labels = km.labels_\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка качества кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(matrix, marked_labels, clustered_labels):\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(marked_labels, clustered_labels))\n",
    "    print(\"Completeness: %0.3f\" % metrics.completeness_score(marked_labels, clustered_labels))\n",
    "    print(\"V-measure: %0.3f\" % metrics.v_measure_score(marked_labels, clustered_labels))\n",
    "    print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(marked_labels, km.labels_))\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(matrix, clustered_labels, sample_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 130 # Количество кластеров\n",
    "n_topics = 100000 # Количество топиков для LDA\n",
    "sampels = 10\n",
    "eps = 1.3\n",
    "n_jobs = 1 # Количество потоков для кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tf_idf(news_content)\n",
    "lda = lda(tfidf_matrix, n_topics)\n",
    "\n",
    "# KMeans tfidf\n",
    "print(\"kmeans tf-idf...\")\n",
    "start_time = time.time()\n",
    "kk_labels = kmeans(tfidf_matrix, n_clusters, n_jobs)\n",
    "score(tfidf_matrix, marked_labels, kk_labels)\n",
    "print(\"kmeans tf-idf: %s second\" % (time.time() - start_time))\n",
    "\n",
    "# KMeans lda\n",
    "print(\"kmeans lda...\")\n",
    "start_time = time.time()\n",
    "kk_labels = kmeans(lda, n_clusters, n_jobs)\n",
    "score(lda, marked_labels, kk_labels)\n",
    "print(\"kmeans lda: %s second\" % (time.time() - start_time))\n",
    "\n",
    "# DBScan tfidf\n",
    "print(\"dbscan tf-idf...\")\n",
    "start_time = time.time()\n",
    "kk_labels = dbscan(tfidf_matrix, eps, sampels, n_jobs)\n",
    "score(tfidf_matrix, marked_labels, kk_labels)\n",
    "print(\"dbscan tf-idf: %s second\" % (time.time() - start_time))\n",
    "\n",
    "# DBScan lda\n",
    "print(\"dbscan lda...\")\n",
    "start_time = time.time()\n",
    "kk_labels = dbscan(lda, eps, sampels, n_jobs)\n",
    "score(lda, marked_labels, kk_labels)\n",
    "print(\"dbscan lda: %s second\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
